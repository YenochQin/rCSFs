# CLAUDE.md

This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.

## Project Overview

rCSFs is a high-performance Rust/Python hybrid library for processing CSF (Configuration State Function) data. It provides two main functionalities:
1. **CSF-to-Parquet conversion**: Convert CSF text files to Parquet format for efficient storage and querying
2. **CSF descriptor generation**: Convert CSF data into fixed-length descriptor arrays for machine learning applications

**Key Implementation Details:**
- Rust edition: 2024 (requires nightly/unstable Rust features, Rust >=1.92.0,<1.93)
- Python support: 3.13 only (strictly pinned in pyproject.toml: `>=3.13,<3.14`)
- Extension module name: `_rcsfs` (the compiled Rust library)
- Public package name: `rcsfs` (Python wrapper in `rcsfs/` at project root)
- Uses pixi for development environment management
- Repository: https://github.com/YenochQin/rCSFs.git

## Build Commands

### Environment Setup
```bash
# Using pixi (recommended)
pixi install
pixi shell

# Or use virtual environment (requires Python 3.13)
python -m venv .venv
source .venv/bin/activate  # Linux/Mac
```

### Development Build
```bash
# Build Rust extension and install in development mode
maturin develop

# Build optimized release version
maturin build --release

# Verify installation
python -c "import rcsfs; print(rcsfs.__version__)"
```

### Testing and Linting
```bash
# Run tests
pytest

# Run tests with speed benchmarking
pytest --speed

# Type checking Python code
mypy python/rcsfs/

# Linting
ruff check python/

# Format code
ruff format python/
```

### Clean Build Artifacts
```bash
# Clean Rust artifacts
cargo clean

# Clean Python build artifacts
rm -rf build/ dist/ *.egg-info/

# Clean maturin cache
maturin clean
```

## Code Architecture

### Module Structure

The project has a two-tier architecture with Rust backend and Python frontend:

**Rust Backend (`src/`):**
- `lib.rs` - PyO3 module definitions, Python exception mappings, and function exports
- `csfs_conversion.rs` - Core CSF-to-Parquet conversion logic (sequential and parallel implementations)
- `csfs_descriptor.rs` - CSF descriptor generation for ML applications

**Python Frontend (`rcsfs/`):**
- `__init__.py` - Public API wrapper that re-exports Rust functions with Python-friendly signatures
- `_rcsfs.pyi` - Type stubs for the compiled Rust extension
- `py.typed` - Marker file indicating this is a typed package for mypy
- `_rcsfs.cpython-313-*.so` - Compiled Rust extension (generated by maturin)

### Key Architectural Patterns

**CSF-to-Parquet Conversion (`csfs_conversion.rs`):**
- **Sequential mode** (`convert_csfs_to_parquet()`): Single-threaded batch processing
- **Parallel mode** (`convert_csfs_to_parquet_parallel()`): Uses `rayon` for parallel batch processing
  - Streaming approach: Read file in chunks to avoid loading large files into memory
  - Rayon's work-stealing automatically distributes work across all CPU cores
  - Maintains CSF order in output (par_iter + collect preserves order)
  - No crossbeam channels - simpler architecture with better cache locality

**CSF Descriptor Generation (`csfs_descriptor.rs`):**
- **CSFDescriptorGenerator**: Parses individual CSF data into fixed-length descriptor arrays
  - Each descriptor contains: `[n_electrons, J_middle, J_coupling]` repeated for each orbital
  - Handles J-value conversion (e.g., "3/2" → 3, "4-" → 8)
- **Parallel batch processing** (`generate_descriptors_from_parquet_parallel()`):
  - Three-stage pipeline: Reader → Workers (Rayon) → Writer
  - Uses `crossbeam-channel` for work distribution
  - Writer uses `BTreeMap` to maintain output order
  - Output: Parquet with single `descriptor: List<int32>` column and ZSTD compression

**Header/Data Separation:**
CSF files have a specific structure:
- First 5 lines: Header metadata (extracted to `{input_stem}_header.toml`)
- Remaining lines: CSF data in 3-line groups (line1, line2, line3)
- Headers are processed first and saved separately from the main Parquet data

**Parquet Output Schemas:**

*CSF-to-Parquet conversion* (uncompressed):
```rust
Schema::new(vec![
    Field::new("idx", DataType::UInt64, false),    // CSF index
    Field::new("line1", DataType::Utf8, false),    // First line of CSF
    Field::new("line2", DataType::Utf8, false),    // Second line
    Field::new("line3", DataType::Utf8, false),    // Third line
])
```

*Descriptor generation* (ZSTD level 3 compression):
```rust
// Single column with variable-length lists
Schema::new(vec![
    Field::new("descriptor", DataType::List(Field::new("item", DataType::Int32, false)), false)
])
```

### Critical Dependencies

**Rust:**
- `pyo3` - Python-Rust bindings and FFI
- `arrow`/`parquet` - Columnar data format and I/O
- `rayon` - Data parallelism (work-stealing thread pool for CSF conversion and descriptor parsing)
- `crossbeam-channel` - Multi-producer multi-consumer channels (descriptor generation pipeline)
- `num_cpus` - CPU core detection for worker thread optimization
- `toml`/`serde` - Header file serialization

**Python:**
- `maturin` - Build tool for Rust-Python extensions
- Build outputs: `.so` files on Linux, `.pyd` on Windows

**Dependency Groups (pyproject.toml):**
- `dev`: pytest, pytest-pretty, pytest-speed
- `lint`: mypy, ruff

**Library Crate Types:**
The `Cargo.toml` configures multiple crate types for flexibility:
- `cdylib` - C dynamic library (for Python extension module)
- `rlib` - Rust library (for Rust consumers)
- `staticlib` - Static library (for static linking)

### Module Naming Disambiguation

This project uses three different names that can be confusing:
- **Cargo package**: `rCSFs` (with capital S and underscore) - in `Cargo.toml`
- **Extension module**: `_rcsfs` (with underscore prefix) - the compiled Rust library, defined in `pyproject.toml` as `module-name = "rcsfs._rcsfs"`
- **Public package**: `rcsfs` (lowercase, no prefix) - the Python wrapper in `rcsfs/` at project root (not `python/rcsfs/`)

### Function/API Mapping

**CSF Conversion:**
- `convert_csfs_to_parquet_parallel()` → `convert_csfs()` / `CSFProcessor.convert()`
- `get_parquet_metadata()` → `get_parquet_info()` / `CSFProcessor.get_metadata()`

**CSF Descriptor Generation:**
- `CSFDescriptorGenerator` (Rust) → `CSFDescriptorGenerator` (Python class)
  - `parse_csf()` - Parse single CSF from 3 lines
  - `batch_parse_csfs()` - Parse multiple CSFs
  - `orbital_count()` - Get number of orbitals
- `generate_descriptors_from_parquet_parallel()` → `generate_descriptors_from_parquet()` (parallel batch processing)
- `read_peel_subshells_from_header()` → `read_peel_subshells()` (extract subshell names from header TOML)

**Class Wrappers:**
- `CSFProcessor` - Object-oriented interface for CSF conversion with configurable `max_line_len` and `chunk_size`

### Release Build Configuration

The `Cargo.toml` specifies optimized release builds:
```toml
[profile.release]
opt-level = 3        # Maximum optimization
lto = true          # Link-time optimization
codegen-units = 1   # Single compilation unit for better optimization
```
Always use `maturin build --release` for production deployments.

## File Naming Conventions

**Output Files:**
- CSF-to-Parquet: User-specified path (e.g., `output.parquet`)
- Header metadata: `{input_file_stem}_header.toml` (auto-generated in same directory as output)
- Descriptors: User-specified path (e.g., `descriptors.parquet`)

**Example:** Converting `data.csf` to `results/output.parquet` generates:
- `results/output.parquet` (CSF data, uncompressed)
- `results/data_header.toml` (header metadata)

## Threading and Performance Considerations

**CSF Conversion (Rayon-based):**
- Default `chunk_size`: 3000000 lines (1M CSFs = 3M lines per batch)
- Worker count defaults to `num_cpus::get()` but is configurable via `num_workers` parameter
- Streaming approach avoids loading entire file into memory
- Progress logging every 100,000 CSFs processed

**Descriptor Generation (Pipeline with Rayon + crossbeam-channel):**
- Three-stage pipeline: Reader thread → Worker threads (Rayon) → Writer thread
- Bounded channel capacity = `num_workers * 2` prevents memory issues
- Worker threads use Rayon's `par_iter` for intra-batch parallelism
- Writer uses `BTreeMap` to maintain output order across asynchronous batches
- Progress logging every 10M CSFs read, 100 batches processed/written

**When to Use Parallel Mode:**
- Large files (>1M CSF entries)
- Multi-core systems available
- Memory-constrained environments (streaming with bounded queues prevents OOM)

## CSF File Format Assumptions

The code assumes CSF files follow this structure:
1. **Lines 1-5**: Header (metadata about the calculation)
2. **Lines 6+**: CSF entries in groups of exactly 3 lines
   - Line 1: CSF identifier/configuration (e.g., "  5s ( 2)  4d-( 4)  4d ( 6)")
   - Line 2: Intermediate J coupling values (e.g., "                   3/2")
   - Line 3: Final coupling and total J value (e.g., "                        4-")

**Validation:** The code processes complete 3-line groups, discarding any incomplete final group.

## CSF Descriptor Format

The descriptor generation converts CSF data into fixed-length arrays for machine learning:

**Descriptor Structure:**
- For each orbital: `[n_electrons, J_middle, J_coupling]` (3 values per orbital)
- Total descriptor size = 3 × number of orbitals
- Unoccupied orbitals have `n_electrons=0`, `J_middle=0`, `J_coupling=final_J`

**J-value Conversion:**
- Fractional J: "3/2" → 3 (the numerator)
- Integer J: "4" → 8 (doubled), "4-" → 8 (parity stripped, then doubled)
- The descriptor stores doubled J values (2J) for integer representation

**Peel Subshells:**
- Extracted from header file line 4 (index 3 in `header_lines`)
- Example: `['5s', '4d-', '4d', '5p-', '5p', '6s']`
- Used to determine descriptor size and orbital ordering
